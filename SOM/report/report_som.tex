\documentclass[a4paper, 12pt]{article}

% Options possibles : 10pt, 11pt, 12pt (taille de la fonte)
% oneside, twoside (recto simple, recto-verso)
% draft, final (stade de développement)

\usepackage[utf8]{inputenc} % LaTeX, comprends les accents !
\usepackage[T1]{fontenc} % Police contenant les caractères franÃ§ais
\usepackage{geometry} % Définir les marges
\usepackage[francais]{babel} % Placez ici une liste de langues, la
% dernière étant la langue principale
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{float}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}

\geometry{left=2.2cm,right=2.2cm,top=2.5cm,bottom=2.5cm}
% \pagestyle{headings} % Pour mettre des entêtes avec les titres
% des sections en haut de page
%\include[dvips]{graphics}

\title{Kohonen Map Project Report \\ \vspace{0.5cm} \large Unsupervised and Reinforcement Learning in Neural Networks}

\author{Han JU, Hao Ren}
\date{\today}

\begin{document}
\maketitle

\section{Learning rate and convergence}
In this part, we start with a Khohonen network of 6x6 neurons and use a Gaussian neighborhood function with standard deviation $\sigma = 3$.

We have tried several learning rate, the observations are show as below:

\begin{enumerate}
\item eta = 0.01 : numbre of iteration = 4
\item eta = 0.001 : numbre of iteration = 24
\item eta = 0.0001 : numbre of iteration = 48
\end{enumerate}

The result is exactly the same to our intuition, small learning rates cause the algorithm to converge slower, while the larger ones help the algorithm to converge faster. However, we can not tell which one is better. As we know, the final solution of the small learning rate will be more stationary, but it might be stuck in local minima, while the large one might make the weights fail to converge.

//figure

For the three learning rate, the resulting figures have no big difference, as the one show above. What is important is that the three learning rates all leads to the convergence of the algorithm. So we can conclude that the bigger learning rate has no side-effect in this project. In addition, according to our observation, smaller learning rates doesn't improve our result. That means the quality of the result depends on other parameters which will be discussed in the later section. As a result, we choose eta = 0.01

In the origin matlab code downloaded from the Moodle, there is a
simple convergence rule which just iterate the algorithm as many times
as the number of the data points. In most of the case, it is not good enough, as some noticeable changes are observed. In order to improve this, we have defined a new convergence condtion which is descibed as following:

\begin{equation}
  \frac{\Vert centers_{n+1} - centers_n\Vert}{\Vert centers_n \Vert}
  < 0.01
\end{equation}

where 0.01 is a converge criterion which can be chosen as needed and each row of the matrix centers is one of the prototypes.

In our convergence rule, every iteration uses all the data points to
calculate the centers. After each iterartion, the convergence
condition will be tested which will decide whether the algorithm will
go on or not. If the change is small enough between the centers of two
successive iterations, we can conclude that the algorithm
converges. Mathmatically, that is to say the norm of the difference of the two centrers matrix is sufficent small (relative to the norm of the previous one).

\section{Visualization and prototypes}
// figure

\section{Assignment method}
In this step, we need a method to automatically assign the propre
digit to each prototype that we found by executing the Kohonen map
algorithm. An obvious idea is to look at the labels of the trainning
data points that are close to the prototype, then decide the its label
accroding to the majority in this surrounding. In fact, this task has
a similarity with the $k$ nearest neighbor classification algorithm,
which assigns a label to a data sample based on the majority of its $k$
nearest neighbors. So we decided to use the kNN method.

Before we apply this method directly to the problem, however, we need
to determine the parameter, $k$, of this algorithm. Intuitively, this
parameter depends on our trainning data, which covers the hand
writing samples of the four chosen digits. So we do firstly a cross
validation on kNN models with distinct $k$ values (ranging from 3 to 300),
which results in $k$ equals 5.

Once the parameter is fixed, the application of kNN to the problem is fairly straightforward.

As a result, we have a matrix which contains the assigned digit to the corresponding prototype of the resulting figure matrix, as following:

//matrix + //figure

\section{$\sigma$ and map size}
In this section we will discuss the role of the width of the neighborhood function and the size of the Kohomen map.

With different $\sigma$ and size of map, we will get different
resulting matrix. In order to descirbe and compare the qualty of the
result map, we come up with a way to define the error rate:

\begin{enumerate}
\item For each data point, we find the prototype which is closest to
  the data point, and assign the digit of the prototype to the data
  point
\item compare the digits of the data points to the labels, if the
  digit is not the same, we count this as an error.
\item finally, we get the error rate: $\dfrac{\#error}{\#dataPoints}$
\end{enumerate}

We have tried $\sigma = 1, 3, 5$ and $sizeK = 6, 7, 8$, the result is showed as table-1:

\begin{table}[!hbp]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
 & $\sigma = 1$ & $\sigma = 3$ & $\sigma = 5$\\
\hline
$sizeK = 6$ &  0.2060 & 0.4455 & 0.7125\\
\hline
$sizeK = 7$ & 0.2295 & 0.3130 & 0.5295\\
\hline
$sizeK = 8$ & 0.1785 & 0.3005 & 0.4900\\
\hline
\end{tabular}
\caption{error rate with different $\sigma$ and $sizeK$}
\end{table}

As a result, the optimal width does not depend on the size of the Kohomen map. For each size of the map, the smaller sigma is , the bette quality the result has. That means the optimal sigma is always the smaller one, no matter how the size of the map changes. With the augment of the size of the Kohonen map, the quaity is improved. An intuitive interpretion of this is that each prototype covers fewer and fewer data points, which makes the prototypes become more and more accurate during the convergence.

In fact, the sigma controls the strength and range of the neighborhood effect of the points of the map. With a small sigma, the algorithm is essentially the competitive learning algorithm, with possible dead units, because the initialization might not be good enough. With big sigmas, all prototypes learn the same thing, i.e. the center of the points.

A propre size of the Kohonen map helps to prevent overifitting. Especially, in the unsupervised learning, it is better to fitting the data with a more or less regular grid that captures the essence of the underlying distribution than using a naive clustering algorithm with so many free parameters.

\section{Conclusion}

In this mini-project we've studied the self-organizing map by applying
this technique to a concrete example. In the meanwhile, we analysed
mainly the convergence condition of the map, as well as the influence of its
different paratmeters to the final result. With this project, we fully
understand the principle of self-organizing maps as a powerful tool in
data analysis.

\end{document}